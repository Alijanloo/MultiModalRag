# Underlying Models

## TableFormer (Table Structure Model)

TableFormer is a **vision-transformer model** for end-to-end table structure recognition[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on). Its **input** is an image of a detected table (cropped from the page) together with the table’s text-grid information. Docling passes the table image (and PDF-extracted cell text) to TableFormer. The model outputs a sequence of HTML-like tokens describing the table structure (e.g. \<table\>, \<thead\>, \<tr\>, \<td colspan=2\>, etc.) **and** bounding boxes for each table cell[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on)[\[2\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20all,amount%20of%20included%20table%20cells). In other words, it predicts how many rows/columns there are, which cells are headers or body, and any row/column spans. Inference outputs are then “matched back” to the PDF’s native text cells so that original text is reused rather than re-OCR’d[\[2\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20all,amount%20of%20included%20table%20cells).

* **Input example:** an image of a 2×3 table (with a row-spanning header). Docling crops the page image to the table region and provides known cell text.

* **Output example:** A structure sequence like \<table\>\<thead\>\<tr\>\<td colspan=3\>Header\</td\>\</tr\>\</thead\>\<tbody\>\<tr\>\<td\>R1C1\</td\>\<td\>R1C2\</td\>\<td\>R1C3\</td\>\</tr\>\<tr\>\<td\>R2C1\</td\>\<td colspan=2\>R2C2-3\</td\>\</tr\>\</tbody\>\</table\> plus bounding boxes for each cell. This encodes that the header spans 3 columns and the second row’s last cell spans 2 columns.

**Datasets:** TableFormer was originally trained on large web-sourced table datasets (e.g. **PubTabNet** with \~227K tables, plus FinTabNet, TableBank, and synthetic tables)[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on). In Docling, it uses pre-trained weights from the TableFormer authors (Nassar et al. 2022[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on) and refinements by Lysak et al. 2023). PubTabNet is explicitly mentioned as a benchmark in related work, but Docling ultimately uses the published TableFormer checkpoints rather than re-training. The model’s custom token language (e.g. HTML tags) was refined in ICDAR 2023[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on).

**Architecture:** TableFormer combines a **CNN backbone** (ResNet-18) with a **Transformer encoder-decoder**. Concretely, the table image (e.g. scaled to 448×448) is passed through ResNet to get feature maps. A Transformer *encoder* processes these features. Two parallel *decoders* then act on the encoder output: (1) a **Structure Decoder** (autoregressive) that generates the sequence of table tags (\<tr\>,\<td\>, etc.), and (2) a **Cell-BBox Decoder** (object-detection style) that outputs a bounding box for each table cell. During training, ground-truth HTML tokens (from PDF) are fed into the structure decoder to guide learning. The architecture handles complex layouts (merged cells, missing borders, nested headers) that older methods struggled with[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on).

**Docling Configuration:** Docling wraps TableFormer as a “table structure recognition” step in the pipeline[\[2\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20all,amount%20of%20included%20table%20cells). By default, Docling uses a *“fast”* TableFormer mode for speed (ResNet-18, fewer layers) but an *“accurate”* mode (larger model) is available[\[3\]](https://docling-project.github.io/docling/usage/advanced_options/#:~:text=Since%20docling%201,quality%20with%20difficult%20table%20structures). It also uses a post-processing “cell-matching” option: by default it **matches** the model’s predicted cell spans back to the PDF’s extracted text cells[\[2\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20all,amount%20of%20included%20table%20cells), which avoids repeating OCR. (This can be turned off to use the model’s own text predictions instead.) Typical tables take a few seconds to process on CPU[\[2\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20all,amount%20of%20included%20table%20cells).

## Layout Analysis – DocLayNet/Heron (RT-DETR Backbone)

Docling’s **layout model** (“Heron”) is an object detector that finds and classifies all page regions (e.g. paragraphs, titles, lists, tables, figures)[\[4\]](https://arxiv.org/html/2408.09869v1#:~:text=Our%20layout%20analysis%20model%20is,5). It is based on the **RT-DETR** Transformer-detection architecture[\[4\]](https://arxiv.org/html/2408.09869v1#:~:text=Our%20layout%20analysis%20model%20is,5). In essence, a page image (rendered at 72 dpi[\[5\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20page,items%2C%20captions%2C%20figures%20or%20tables)) is fed into a ResNet backbone and a DETR-style Transformer. The model predicts a set of bounding boxes and class labels for each element on the page. For example, on a scientific paper page it might output objects like “Title” at (x1,y1,x2,y2), “Text” blocks, “List-Item”, “Caption”, “Picture” (for figures), and “Table”[\[6\]](https://huggingface.co/ds4sd/docling-layout-heron#:~:text=classes_map%20%3D%20%7B%200%3A%20,header)[\[5\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20page,items%2C%20captions%2C%20figures%20or%20tables). Docling then intersects these predicted boxes with the PDF’s text tokens to form higher-level groups (e.g. assigning words to paragraphs, linking figures to captions)[\[5\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20page,items%2C%20captions%2C%20figures%20or%20tables).

* **Input example:** a full page image (e.g. 800×1000px PNG at 72 dpi).

* **Output example:** A set of detected regions, e.g.:

* *Title*: bbox \= \[50,50–450,100\],

* *Text*: bbox \= \[50,110–450,600\] (body paragraph),

* *Figure*: bbox \= \[50,620–450,900\] (an embedded image),

* *Table*: bbox \= \[480,50–780,350\] (a table), and so on. Each has a confidence score.

**Datasets:** The Heron model was **trained on DocLayNet** (Pfitzmann et al. 2022), a large human-annotated layout dataset (\~80k pages with 11 original classes like Title, Text, List-item, Caption, etc.)[\[4\]](https://arxiv.org/html/2408.09869v1#:~:text=Our%20layout%20analysis%20model%20is,5)[\[6\]](https://huggingface.co/ds4sd/docling-layout-heron#:~:text=classes_map%20%3D%20%7B%200%3A%20,header). Docling has also incorporated an expanded set of classes (up to 17\) via additional data (DocLayNet-v2 and similar corpora[\[7\]](https://arxiv.org/html/2509.11720v1/#:~:text=unifies%20a%20post,The)). The HuggingFace model card lists classes like *Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title* (and extras like “Document Index”, “Form”, etc.)[\[6\]](https://huggingface.co/ds4sd/docling-layout-heron#:~:text=classes_map%20%3D%20%7B%200%3A%20,header)[\[8\]](https://huggingface.co/ds4sd/docling-layout-heron#:~:text=10%3A%20,Value%20Region%22%2C).

**Architecture:** Docling’s default **“heron”** model uses **RT-DETRv2 with a ResNet-50 backbone (42.9M parameters)**[\[9\]](https://arxiv.org/html/2509.11720v1/#:~:text=egret,r101vd%2076.7). (An “heron-101” variant uses ResNet-101, 76.7M params, for slightly higher accuracy.) RT-DETR (Real-Time DETR) is a recently proposed DETR variant optimized for speed[\[10\]](https://arxiv.org/html/2509.11720v1/#:~:text=Within%20the%20DETR%20model%20family,configurations%3A%20medium%2C%20large%2C%20and%20xlarge)[\[9\]](https://arxiv.org/html/2509.11720v1/#:~:text=egret,r101vd%2076.7). It retains the Transformer encoder-decoder core of DETR but includes engineering improvements (e.g. efficient query selection) to run faster. In Docling, inference runs via ONNX Runtime for efficiency[\[4\]](https://arxiv.org/html/2408.09869v1#:~:text=Our%20layout%20analysis%20model%20is,5). Post-processing filters out overlapping boxes (keeping the highest-scoring) and snaps each box to the nearest underlying PDF cells[\[5\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20page,items%2C%20captions%2C%20figures%20or%20tables).

**Docling Configuration:** By default, Docling sets up the layout model (Heron) in its pipeline options (under layout\_options.model\_spec \= DOCLING\_LAYOUT\_HERON[\[11\]](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.EasyOcrOptions#:~:text=%23%20%20%60%60%20model_spec%20%60class,attribute)). The model operates on the low-resolution (72 dpi) page images to save time, yet achieves high recall. Docling’s code then matches each detected “regular” element with PDF text cells[\[5\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20page,items%2C%20captions%2C%20figures%20or%20tables). The result is that paragraphs, section titles, lists, captions, etc., are assembled from the raw detections. In practice, the Heron model dramatically improves layout mAP over earlier detectors (78% mAP on canonical DocLayNet)[\[12\]](https://arxiv.org/html/2509.11720v1/#:~:text=heron%20docling%200,072), with sub-second CPU inference per page.

## EasyOCR (OCR Engine)

Docling integrates **EasyOCR** (an open-source PyTorch OCR library) for text recognition on scanned or bitmap content[\[13\]](https://arxiv.org/html/2408.09869v1#:~:text=Docling%20provides%20optional%20support%20for,of%2030%20seconds%20per%20page). EasyOCR uses a modern deep-learning pipeline: first a **CRAFT** network detects text regions, then a **CRNN** (CNN+RNN) model reads characters[\[14\]](https://adityamangal98.medium.com/tesseract-ocr-vs-easyocr-a-deep-architectural-duel-between-the-old-and-the-new-0f52f7fd32d2#:~:text=1,Character%20Region%20Awareness%20for%20Text)[\[15\]](https://github.com/JaidedAI/EasyOCR#:~:text=Detection%20execution%20uses%20the%20CRAFT,is%20provided%20by%20%2098). In Docling, by default each page is rendered at **216 dpi** (much higher than layout’s 72 dpi) and fed to EasyOCR to capture small fonts[\[13\]](https://arxiv.org/html/2408.09869v1#:~:text=Docling%20provides%20optional%20support%20for,of%2030%20seconds%20per%20page). The output is a list of (bounding box, text) pairs for each line or word. For example, given a high-res image of the text “Hello World”, EasyOCR might output \[(\[x1,y1,x2,y2\], "Hello"), (\[x3,y3,x4,y4\], "World")\].

* **Datasets/Models:** EasyOCR uses pre-trained models shipped by JaidedAI. It does not require Docling-specific training; rather, it leverages large multilingual OCR training corpora. (Docling simply calls easyocr.Reader under the hood.)

* **Architecture:** EasyOCR’s detection stage is **CRAFT** (Character Region Awareness for Text)[\[14\]](https://adityamangal98.medium.com/tesseract-ocr-vs-easyocr-a-deep-architectural-duel-between-the-old-and-the-new-0f52f7fd32d2#:~:text=1,Character%20Region%20Awareness%20for%20Text), and its recognition stage is **CRNN** (Convolutional-Recurrent Neural Network)[\[14\]](https://adityamangal98.medium.com/tesseract-ocr-vs-easyocr-a-deep-architectural-duel-between-the-old-and-the-new-0f52f7fd32d2#:~:text=1,Character%20Region%20Awareness%20for%20Text)[\[15\]](https://github.com/JaidedAI/EasyOCR#:~:text=Detection%20execution%20uses%20the%20CRAFT,is%20provided%20by%20%2098). The CRNN typically uses a ResNet+LSTM stack with CTC decoding. In code, Docling’s EasyOCR pipeline can use GPU if available. The EasyOcrOptions include a recog\_network parameter (default “standard” CNN), and an optional use\_gpu flag[\[16\]](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.EasyOcrOptions#:~:text=%23%20%20%60%60%20recog_network%20%60class,attribute).

* **Docling Configuration:** By default Docling’s do\_ocr=True and uses EasyOcrOptions()[\[13\]](https://arxiv.org/html/2408.09869v1#:~:text=Docling%20provides%20optional%20support%20for,of%2030%20seconds%20per%20page). The default languages are \['fr','de','es','en'\] (English, French, German, Spanish)[\[17\]](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.EasyOcrOptions#:~:text=%23%20%20%60%60%20lang%20%60class,attribute), though this can be customized. The OCR step can be limited to areas where programmatic text is missing; Docling allows “force\_full\_page\_ocr” to run OCR on the entire page if needed. Because EasyOCR is relatively slow on CPU (≳30 sec/page[\[13\]](https://arxiv.org/html/2408.09869v1#:~:text=Docling%20provides%20optional%20support%20for,of%2030%20seconds%20per%20page)), Docling offers alternatives (Tesseract, etc.) but EasyOCR gives the best multi-language accuracy.

**Example:** For a scanned page image at 216 dpi, Docling would run EasyOCR and obtain, say, *“Figure 1: Architecture of Transformer-based table model”* as recognized lines with their bounding polygons. These text strings are then merged with any programmatic PDF text during final assembly.

**Sources:** The above details come from the Docling technical report and documentation[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on)[\[2\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20all,amount%20of%20included%20table%20cells)[\[4\]](https://arxiv.org/html/2408.09869v1#:~:text=Our%20layout%20analysis%20model%20is,5)[\[5\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20page,items%2C%20captions%2C%20figures%20or%20tables)[\[13\]](https://arxiv.org/html/2408.09869v1#:~:text=Docling%20provides%20optional%20support%20for,of%2030%20seconds%20per%20page). For instance, the report explicitly states that Docling “relies on EasyOCR” at 216 dpi for OCR[\[13\]](https://arxiv.org/html/2408.09869v1#:~:text=Docling%20provides%20optional%20support%20for,of%2030%20seconds%20per%20page), and that the layout model “is derived from RT-DETR and re-trained on DocLayNet”[\[4\]](https://arxiv.org/html/2408.09869v1#:~:text=Our%20layout%20analysis%20model%20is,5). Tables of model specs and class labels are from the Docling model repository and related papers[\[9\]](https://arxiv.org/html/2509.11720v1/#:~:text=egret,r101vd%2076.7)[\[6\]](https://huggingface.co/ds4sd/docling-layout-heron#:~:text=classes_map%20%3D%20%7B%200%3A%20,header).

---

[\[1\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20TableFormer%20model%C2%A0,inference%2C%20our%20implementation%20relies%20on) [\[2\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20all,amount%20of%20included%20table%20cells) [\[4\]](https://arxiv.org/html/2408.09869v1#:~:text=Our%20layout%20analysis%20model%20is,5) [\[5\]](https://arxiv.org/html/2408.09869v1#:~:text=The%20Docling%20pipeline%20feeds%20page,items%2C%20captions%2C%20figures%20or%20tables) [\[13\]](https://arxiv.org/html/2408.09869v1#:~:text=Docling%20provides%20optional%20support%20for,of%2030%20seconds%20per%20page) Docling Technical Report

[https://arxiv.org/html/2408.09869v1](https://arxiv.org/html/2408.09869v1)

[\[3\]](https://docling-project.github.io/docling/usage/advanced_options/#:~:text=Since%20docling%201,quality%20with%20difficult%20table%20structures) Advanced options \- Docling

[https://docling-project.github.io/docling/usage/advanced\_options/](https://docling-project.github.io/docling/usage/advanced_options/)

[\[6\]](https://huggingface.co/ds4sd/docling-layout-heron#:~:text=classes_map%20%3D%20%7B%200%3A%20,header) [\[8\]](https://huggingface.co/ds4sd/docling-layout-heron#:~:text=10%3A%20,Value%20Region%22%2C) ds4sd/docling-layout-heron · Hugging Face

[https://huggingface.co/ds4sd/docling-layout-heron](https://huggingface.co/ds4sd/docling-layout-heron)

[\[7\]](https://arxiv.org/html/2509.11720v1/#:~:text=unifies%20a%20post,The) [\[9\]](https://arxiv.org/html/2509.11720v1/#:~:text=egret,r101vd%2076.7) [\[10\]](https://arxiv.org/html/2509.11720v1/#:~:text=Within%20the%20DETR%20model%20family,configurations%3A%20medium%2C%20large%2C%20and%20xlarge) [\[12\]](https://arxiv.org/html/2509.11720v1/#:~:text=heron%20docling%200,072) Advanced Layout Analysis Models for Docling

[https://arxiv.org/html/2509.11720v1/](https://arxiv.org/html/2509.11720v1/)

[\[11\]](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.EasyOcrOptions#:~:text=%23%20%20%60%60%20model_spec%20%60class,attribute) [\[16\]](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.EasyOcrOptions#:~:text=%23%20%20%60%60%20recog_network%20%60class,attribute) [\[17\]](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.EasyOcrOptions#:~:text=%23%20%20%60%60%20lang%20%60class,attribute) Pipeline options \- Docling

[https://docling-project.github.io/docling/reference/pipeline\_options/](https://docling-project.github.io/docling/reference/pipeline_options/)

[\[14\]](https://adityamangal98.medium.com/tesseract-ocr-vs-easyocr-a-deep-architectural-duel-between-the-old-and-the-new-0f52f7fd32d2#:~:text=1,Character%20Region%20Awareness%20for%20Text) Tesseract OCR vs EasyOCR: A Deep Architectural Duel Between the Old and the New | by Aditya Mangal | Medium

[https://adityamangal98.medium.com/tesseract-ocr-vs-easyocr-a-deep-architectural-duel-between-the-old-and-the-new-0f52f7fd32d2](https://adityamangal98.medium.com/tesseract-ocr-vs-easyocr-a-deep-architectural-duel-between-the-old-and-the-new-0f52f7fd32d2)

[\[15\]](https://github.com/JaidedAI/EasyOCR#:~:text=Detection%20execution%20uses%20the%20CRAFT,is%20provided%20by%20%2098) GitHub \- JaidedAI/EasyOCR: Ready-to-use OCR with 80+ supported languages and all popular writing scripts including Latin, Chinese, Arabic, Devanagari, Cyrillic and etc.

[https://github.com/JaidedAI/EasyOCR](https://github.com/JaidedAI/EasyOCR)